{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 文字レベルでの言語モデル\n",
    "\n",
    "このチュートリアルでは、複数レイヤのリカレントニューラルネットワークで構成される、文字レベルでの言語モデルをどのように学習するかを見ていきます。特にここでは、文章を生成できる複数レイヤの LSTM ネットワークを取り上げましょう。\n",
    "\n",
    "## データの準備\n",
    "\n",
    "まず、適当な PDF を `data` フォルダにダウンロードします。  \n",
    "そして以下のシェルスクリプトを実行して、データを整形しましょう。\n",
    "\n",
    "```\n",
    "./development/generate-data.sh\n",
    "```\n",
    "\n",
    "その上で、その中身がどんなものであるかを確認してみます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with open('./src/train/input.txt', 'r') as f:\n",
    "    print f.read()[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "ここで前処理のための便利関数をいくつか定義しておきます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def read_content(path):\n",
    "    with open(path) as ins:        \n",
    "        return ins.read()\n",
    "    \n",
    "# Return a dict which maps each char into an unique int id\n",
    "def build_vocab(path):\n",
    "    content = list(read_content(path))\n",
    "    idx = 1 # 0 is left for zero-padding\n",
    "    the_vocab = {}\n",
    "    for word in content:\n",
    "        if len(word) == 0:\n",
    "            continue\n",
    "        if not word in the_vocab:\n",
    "            the_vocab[word] = idx\n",
    "            idx += 1\n",
    "    return the_vocab\n",
    "\n",
    "# Encode a sentence with int ids\n",
    "def text2id(sentence, the_vocab):\n",
    "    words = list(sentence)\n",
    "    return [the_vocab[w] for w in words if len(w) > 0]\n",
    "            \n",
    "# build char vocabluary from input\n",
    "vocab = build_vocab(\"./src/train/input.txt\")\n",
    "print('vocab size = %d' %(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## LSTM モデルの生成\n",
    "\n",
    "ではいよいよ、複数レイヤの LSTM モデルを作りましょう。LSTM セルの定義は本家のものをそのまま使っています [lstm.py](https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/lstm.py)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src/train')\n",
    "import lstm\n",
    "\n",
    "# Each line contains at most 129 chars. \n",
    "seq_len = 129\n",
    "# embedding dimension, which maps a character to a 256-dimension vector\n",
    "num_embed = 256\n",
    "# number of lstm layers\n",
    "num_lstm_layer = 3\n",
    "# hidden unit in LSTM cell\n",
    "num_hidden = 512\n",
    "\n",
    "symbol = lstm.lstm_unroll(\n",
    "    num_lstm_layer, \n",
    "    seq_len,\n",
    "    len(vocab) + 1,\n",
    "    num_hidden=num_hidden,\n",
    "    num_embed=num_embed,\n",
    "    num_label=len(vocab) + 1, \n",
    "    dropout=0.2)\n",
    "\n",
    "\"\"\"test_seq_len\"\"\"\n",
    "data_file = open(\"./src/train/input.txt\")\n",
    "for line in data_file:\n",
    "    assert len(line) <= seq_len + 1, \"seq_len is smaller than maximum line length. Current line length is %d. Line content is: %s\" % (len(line), line)\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## 学習\n",
    "\n",
    "まず、イテレータを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import bucket_io\n",
    "\n",
    "# The batch size for training\n",
    "batch_size = 32\n",
    "\n",
    "# initalize states for LSTM\n",
    "init_c = [('l%d_init_c'%l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n",
    "init_h = [('l%d_init_h'%l, (batch_size, num_hidden)) for l in range(num_lstm_layer)]\n",
    "init_states = init_c + init_h\n",
    "\n",
    "# Even though BucketSentenceIter supports various length examples,\n",
    "# we simply use the fixed length version here\n",
    "data_train = bucket_io.BucketSentenceIter(\n",
    "    \"./src/train/input.txt\", \n",
    "    vocab, \n",
    "    [seq_len], \n",
    "    batch_size,             \n",
    "    init_states, \n",
    "    seperate_char='\\n',\n",
    "    text2id=text2id, \n",
    "    read_content=read_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "これで標準的な `model.fit` アプローチを使って学習できます。  \n",
    "（MXNet の古い手法で書いているため deprecated と出ますが、先に進むことはできます）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# @@@ AUTOTEST_OUTPUT_IGNORED_CELL\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.DEBUG)\n",
    "\n",
    "# We will show a quick demo with only 1 epoch. In practice, we can set it to be 100\n",
    "num_epoch = 1\n",
    "# learning rate \n",
    "learning_rate = 0.01\n",
    "\n",
    "# Evaluation metric\n",
    "def Perplexity(label, pred):\n",
    "    loss = 0.\n",
    "    for i in range(pred.shape[0]):\n",
    "        loss += -np.log(max(1e-10, pred[i][int(label[i])]))\n",
    "    return np.exp(loss / label.size)\n",
    "\n",
    "model = mx.model.FeedForward(\n",
    "    ctx=mx.cpu(),\n",
    "    symbol=symbol,\n",
    "    num_epoch=num_epoch,\n",
    "    learning_rate=learning_rate,\n",
    "    momentum=0,\n",
    "    wd=0.0001,\n",
    "    initializer=mx.init.Xavier(factor_type=\"in\", magnitude=2.34))\n",
    "\n",
    "model.fit(X=data_train,\n",
    "          eval_metric=mx.metric.np(Perplexity),\n",
    "          batch_end_callback=mx.callback.Speedometer(batch_size, 20),\n",
    "          epoch_end_callback=mx.callback.do_checkpoint(\"model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 推論\n",
    "\n",
    "推論をサポートするいくつかの関数を定義しておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './src/inference')\n",
    "from rnn_model import LSTMInferenceModel\n",
    "\n",
    "# helper strcuture for prediction\n",
    "def MakeRevertVocab(vocab):\n",
    "    dic = {}\n",
    "    for k, v in vocab.items():\n",
    "        dic[v] = k\n",
    "    return dic\n",
    "\n",
    "# make input from char\n",
    "def MakeInput(char, vocab, arr):\n",
    "    idx = vocab[char]\n",
    "    tmp = np.zeros((1,))\n",
    "    tmp[0] = idx\n",
    "    arr[:] = tmp\n",
    "\n",
    "# helper function for random sample \n",
    "def _cdf(weights):\n",
    "    total = sum(weights)\n",
    "    result = []\n",
    "    cumsum = 0\n",
    "    for w in weights:\n",
    "        cumsum += w\n",
    "        result.append(cumsum / total)\n",
    "    return result\n",
    "\n",
    "def _choice(population, weights):\n",
    "    assert len(population) == len(weights)\n",
    "    cdf_vals = _cdf(weights)\n",
    "    x = random.random()\n",
    "    idx = bisect.bisect(cdf_vals, x)\n",
    "    return population[idx]\n",
    "\n",
    "# we can use random output or fixed output by choosing largest probability\n",
    "def MakeOutput(prob, vocab, sample=False, temperature=1.):\n",
    "    if sample == False:\n",
    "        idx = np.argmax(prob, axis=1)[0]\n",
    "    else:\n",
    "        fix_dict = [\"\"] + [vocab[i] for i in range(1, len(vocab) + 1)]\n",
    "        scale_prob = np.clip(prob, 1e-6, 1 - 1e-6)\n",
    "        rescale = np.exp(np.log(scale_prob) / temperature)\n",
    "        rescale[:] /= rescale.sum()\n",
    "        return _choice(fix_dict, rescale[0, :])\n",
    "    try:\n",
    "        char = vocab[idx]\n",
    "    except:\n",
    "        char = ''\n",
    "    return char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "事前に学習したモデルからパラメタを読み込み、推論のためのモデルを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "import rnn_model \n",
    "\n",
    "# load from check-point\n",
    "_, arg_params, __ = mx.model.load_checkpoint(\"model\", num_epoch)\n",
    "\n",
    "# build an inference model\n",
    "model = rnn_model.LSTMInferenceModel(\n",
    "    num_lstm_layer,\n",
    "    len(vocab) + 1,\n",
    "    num_hidden=num_hidden,\n",
    "    num_embed=num_embed,\n",
    "    num_label=len(vocab) + 1, \n",
    "    arg_params=arg_params, \n",
    "    ctx=mx.cpu(), \n",
    "    dropout=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "準備ができました。\"Amazon EC2 is \" から始まる 300 文字を生成してみましょう！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import bisect\n",
    "\n",
    "seq_length = 300\n",
    "input_ndarray = mx.nd.zeros((1,))\n",
    "revert_vocab = MakeRevertVocab(vocab)\n",
    "\n",
    "# Feel free to change the starter sentence\n",
    "output ='Amazon EC2 is '\n",
    "\n",
    "random_sample = True\n",
    "new_sentence = True\n",
    "\n",
    "ignore_length = len(output)\n",
    "\n",
    "for i in range(seq_length):\n",
    "    if i <= ignore_length - 1:\n",
    "        MakeInput(output[i], vocab, input_ndarray)\n",
    "    else:\n",
    "        MakeInput(output[-1], vocab, input_ndarray)\n",
    "    prob = model.forward(input_ndarray, new_sentence)\n",
    "    new_sentence = False\n",
    "    next_char = MakeOutput(prob, revert_vocab, random_sample)\n",
    "    if next_char == '':\n",
    "        new_sentence = True\n",
    "    if i >= ignore_length - 1:\n",
    "        output += next_char\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
